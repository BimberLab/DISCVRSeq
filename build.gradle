// Dependencies for the buildscript (not the program)
buildscript {
    repositories {
        mavenCentral()
        jcenter() // for shadow plugin
    }
}

plugins {
    id "java"
    id "application"
    id "com.github.johnrengelman.shadow" version "5.0.0"    //used to build the shadow and sparkJars
    id 'com.palantir.git-version' version '0.5.1' //version helper
}

import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar

import javax.tools.ToolProvider

apply plugin: 'java'

repositories {
    mavenCentral()
    jcenter()
    maven {
        url "https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot-local/"
    }
}

// This if -dRelease=true is passed, this should return only the last tag (i.e. 1.0).  
// Otherwise it should return the tag and hash, i.e.: 1.08-7-g72826dd
final isRelease = Boolean.getBoolean("release")
final details = versionDetails()
version = (isRelease ? details.lastTag : gitVersion() + "-SNAPSHOT").replaceAll(".dirty", "")

logger.info("build for version:" + version)

//NOTE: we ignore contracts for now
compileJava {
    options.compilerArgs = ['-proc:none', '-Xlint:all', '-Werror', '-Xdiags:verbose']
}
compileTestJava {
    options.compilerArgs = ['-proc:none', '-Xlint:all', '-Werror', '-Xdiags:verbose']
}

configurations {
    externalSourceConfiguration {
        // External sources we need for doc and tab completion generation tasks (i.e., Picard sources)
        transitive false
    }
}

mainClassName = "com.github." + rootProject.name.toLowerCase() + ".Main"

//see this thread: https://github.com/broadinstitute/gatk/issues/2300#issuecomment-333627036
final gatkVersion = '4.1.4.1'
final htsjdkVersion = System.getProperty('htsjdk.version','2.21.1')
final barclayVersion = System.getProperty('barclay.version','2.1.0')
final testNGVersion = '7.0.0'
final googleCloudNioDependency = 'com.google.cloud:google-cloud-nio:0.107.0-alpha:shaded'

final docBuildDir = "$buildDir/docs"
logger.info(docBuildDir)

configurations.all {
    resolutionStrategy {
        // force the htsjdk version so we don't get a different one transitively or GATK
        force 'com.github.samtools:htsjdk:' + htsjdkVersion
        // later versions explode Hadoop
        // TODO: this is the same in GATK, but we should check if they solve this issue in the future
        force 'com.google.protobuf:protobuf-java:3.0.0-beta-1'
        // force testng dependency so we don't pick up a different version via GenomicsDB/GATK
        force 'org.testng:testng:' + testNGVersion
        force 'org.broadinstitute:barclay:' + barclayVersion

        // make sure we don't pick up an incorrect version of the GATK variant of the google-nio library
        // via Picard, etc.
        force googleCloudNioDependency
    }
}

// Get the jdk files we need to run javaDoc. We need to use these during compile, testCompile,
// test execution, and toolDoc generation, but we don't want them as part of the runtime
// classpath and we don't want to redistribute them in the uber jar.
final javadocJDKFiles = files(((URLClassLoader) ToolProvider.getSystemToolClassLoader()).getURLs())

dependencies {
    // javadoc utilities; compile/test only to prevent redistribution of sdk jars
    compileOnly(javadocJDKFiles)
    testCompile(javadocJDKFiles)

    compile 'org.broadinstitute:barclay:' + barclayVersion

    // use the same GATK dependency for compile and documentation
    final gatkDependency = 'org.broadinstitute:gatk:' + gatkVersion
    compile (gatkDependency) {
        exclude module: 'jgrapht' // this is not required
    }
    externalSourceConfiguration 'org.broadinstitute:gatk:' + gatkVersion + ':sources'

    compile group: 'org.broadinstitute', name: 'gatk-test-utils', version: gatkVersion
    compile group: 'com.github.samtools', name: 'htsjdk', version: htsjdkVersion
    compile googleCloudNioDependency

    // Required for SV Discovery machine learning
    compile group: 'biz.k11i', name: 'xgboost-predictor', version: '0.3.0'

    // this comes built-in when running on Google Dataproc, but the library
    // allows us to read from GCS also when testing locally (or on non-Dataproc clusters,
    // should we want to)
    compile 'com.google.cloud.bigdataoss:gcs-connector:1.6.3-hadoop2'

    compile group: 'com.milaboratory', name: 'milib', version: '1.11'
    
    // compilation for testing
    testCompile 'org.testng:testng:' + testNGVersion

    // https://mvnrepository.com/artifact/org.apache.commons/commons-text
    compile group: 'org.apache.commons', name: 'commons-text', version: '1.8'

    compile 'org.biojava:biojava-core:5.3.0'
}

wrapper {
    gradleVersion = '5.6'
}

tasks.withType(Jar) {
    manifest {
        attributes 'Implementation-Title': 'DISCVR-seq Toolkit',
                'Implementation-Version': version,
                'Main-Class': mainClassName,
                'GATK-Version': gatkVersion,
                'htsjdk-Version': htsjdkVersion
    }
}

tasks.withType(ShadowJar) {
    from(project.sourceSets.main.output)
    baseName = project.name
    mergeServiceFiles()
    relocate 'com.google.common', 'org.broadinstitute.hellbender.relocated.com.google.common'
    zip64 true
    exclude 'log4j.properties' // from adam jar as it clashes with hellbender's log4j2.xml
    exclude '**/*.SF' // these are Manifest signature files and
    exclude '**/*.RSA' // keys which may accidentally be imported from other signed projects and then fail at runtime

    // Suggested by the akka devs to make sure that we do not get the spark configuration error.
    // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler
    transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {
        resource = 'reference.conf'
    }
}

shadowJar {
    configurations = [project.configurations.runtime]
    baseName = project.name
    classifier = ''
    mergeServiceFiles('reference.conf')
}

task localJar{ dependsOn shadowJar }

task sourcesJar(type: Jar) {
    from sourceSets.main.allSource
    classifier = 'sources'
}

tasks.withType(Javadoc) {
    // do this for all javadoc tasks, including toolDoc
    options.addStringOption('Xdoclint:none')
    outputs.upToDateWhen { false }
}

javadoc {
    // This is a hack to disable the java 8 default javadoc lint until we fix the html formatting
    // We only want to do this for the javadoc task, not toolDoc
    options.addStringOption('Xdoclint:none', '-quiet')
    source = sourceSets.main.allJava + files(configurations.externalSourceConfiguration.collect { zipTree(it) })
    include '**com/github/discvrseq/**/*.java'
    include '**org/broadinstitute/hellbender/cmdline/argumentcollections/**/*.java'
    include '**org/broadinstitute/hellbender/cmdline/**/*.java'
}

// Generate Online Doc
task toolDoc(type: Javadoc, dependsOn: classes ) {
    final File baseDocDir = new File("build/docs")
    final File toolDocDir = new File("build/docs/toolDoc")
    doFirst {
        // make sure the output folder exists or we can create it
        if (!toolDocDir.exists() && !toolDocDir.mkdirs()) {
            throw new GradleException(String.format("Failure creating folder (%s) for HTML doc output in task (%s)",
                    toolDocDir.getAbsolutePath(),
                    it.name));
        }
        copy {
            from('src/main/resources/com/github/discvrseq/util/docTemplates')
            include 'stylesheet.css'
            include 'images/**'
            include 'resources/**'
            into toolDocDir
        }
        copy {
            from('src/test/resources/com/github/discvrseq/TestData')
            include 'SimpleExample.vcf.gz'
            include 'SimpleExample.vcf.gz.tbi'
            into toolDocDir
        }
        copy {
            from('src/main/resources/com/github/discvrseq/util/docTemplates')
            include 'index.md'
            include 'README.md'
            include '_config.yml'
            into baseDocDir
        }
    }

    source = sourceSets.main.allJava + files(configurations.externalSourceConfiguration.collect { zipTree(it) })
    include '**com/github/discvrseq/**/*.java'
    include '**org/broadinstitute/hellbender/cmdline/argumentcollections/**/*.java'
    include '**org/broadinstitute/hellbender/cmdline/**/*.java'

    // The doc process instantiates any documented feature classes, so to run it we need the entire
    // runtime classpath, as well as jdk javadoc files such as tools.jar, where com.sun.javadoc lives.
    classpath = sourceSets.main.runtimeClasspath + javadocJDKFiles
    
    options.docletpath = classpath.asType(List)
    options.doclet = "com.github.discvrseq.util.help.DISCVRSeqHelpDoclet"

    outputs.dir(toolDocDir)
    options.destinationDirectory(toolDocDir)

    options.addStringOption("settings-dir", "src/main/resources/com/github/discvrseq/util/docTemplates");
    options.addStringOption("absolute-version", getVersion())
    options.addStringOption("build-timestamp", new Date().format("dd-mm-yyyy hh:mm:ss"))
}

test {
    outputs.upToDateWhen { false }  //tests will never be "up to date" so you can always rerun them
    String TEST_VERBOSITY = "$System.env.TEST_VERBOSITY"

    useTestNG {

    }

    systemProperty "samjdk.use_async_io_read_samtools", "false"
    systemProperty "samjdk.use_async_io_write_samtools", "true"
    systemProperty "samjdk.use_async_io_write_tribble", "false"
    systemProperty "samjdk.compression_level", "1"
    systemProperty "gatk.spark.debug", System.getProperty("gatk.spark.debug")

    // set heap size for the test JVM(s)
    minHeapSize = "1G"
    maxHeapSize = "4G"

    if (TEST_VERBOSITY == "minimal") {
        int count = 0
        // listen to events in the test execution lifecycle

        beforeTest { descriptor ->
            count++
            if( count % 10000 == 0) {
                logger.lifecycle("Finished "+ Integer.toString(count++) + " tests")
            }
        }
    } else {
        // show standard out and standard error of the test JVM(s) on the console
        testLogging.showStandardStreams = true
        beforeTest { descriptor ->
            logger.lifecycle("Running Test: " + descriptor)
        }

        // listen to standard out and standard error of the test JVM(s)
        onOutput { descriptor, event ->
            logger.lifecycle("Test: " + descriptor + " produced standard out/err: " + event.message )
        }
    }

    testLogging {
        testLogging {
            events "skipped", "failed"
            exceptionFormat = "full"
        }
        afterSuite { desc, result ->
            if (!desc.parent) { // will match the outermost suite
                println "Results: ${result.resultType} (${result.testCount} tests, ${result.successfulTestCount} successes, ${result.failedTestCount} failures, ${result.skippedTestCount} skipped)"
            }
        }
    }
}
